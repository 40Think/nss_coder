# VoicePal v3 Architecture

**Voice-to-Specification System with Recursive Hypothesis-Driven Search**

> âš¡ **MIGRATION (2025-12-16)**: Whisper backend upgraded from whisper.cpp to **faster-whisper**
> - Performance: **82.7x real-time** on RTX PRO 6000 Blackwell
> - 80 minutes of audio processed in 1 minute
> - See: `voice_whisper_fast.py`

---

## ğŸ“Š Visual Architecture

![VoicePal Architecture](file:///home/user/Telegram_Parser/docs/automation/voicepal_architecture.png)

---

## ğŸ”„ System Flow

### Phase 1: Voice Input & Enhancement
```
Voice Audio â†’ faster-whisper (GPU, 82.7x real-time) â†’ Raw Text â†’ Enhancement (vLLM) â†’ Enhanced Text
             â†‘ MIGRATED 2025-12-16 from whisper.cpp
```

### Phase 2: Total Recall (Round 1)
```
Enhanced Text â†’ Binary LLM Classification (64 parallel)
    â†“
Scan ALL files (1,254 files in ~25s)
    â†“
YES/NO per file â†’ 5-20 relevant files
```

### Phase 3: Hypothesis Generation (Round 2)
```
Relevant Files â†’ Generate 10 Hypotheses
    â†“
User Selects Hypotheses
    â†“
File Mapping â†’ Refined Search
```

### Phase 4: Dependency Analysis
```
Selected Files â†’ Extract Dependencies
    â†“
Batch Read (64 concurrent)
    â†“
Imports + Callers + Tags + Folder Context
```

### Phase 5: Output Generation
```
Context + Dependencies â†’ Parallel Generation
    â”œâ”€ Specification (Markdown)
    â”œâ”€ Tickets (10 sub-tasks)
    â””â”€ Documentation
```

### Phase 6: Multi-Agent Execution (Optional)
```
Approved Spec â†’ Task Decomposition
    â†“
10 Sub-tasks â†’ 10 Parallel Agents
    â†“
Code Generation + Logging
```

---

## ğŸ“ File Structure

```
docs/automation/
â”œâ”€â”€ voice_server.py              # Flask server + UI
â”œâ”€â”€ voice_processor.py           # Processing pipeline
â”œâ”€â”€ voice_whisper.py             # Whisper integration (LEGACY - whisper.cpp)
â”œâ”€â”€ voice_whisper_fast.py        # Whisper integration (CURRENT - faster-whisper, 82.7x)
â”œâ”€â”€ whisper_fast.sh              # Wrapper script for LD_LIBRARY_PATH
â”œâ”€â”€ voicepal_architecture.mmd    # Mermaid diagram
â”œâ”€â”€ voicepal_architecture.png    # Visual representation
â””â”€â”€ VOICEPAL_INNOVATION_ANALYSIS.md  # Detailed analysis
```

---

## ğŸ”— Dependencies

### Input Dependencies
- **Audio**: User voice recordings
- **Obsidian Vault**: Ideas/intuitions (optional)
- **Web Search**: External context (optional)

### Core Dependencies
- **faster-whisper**: GPU-accelerated transcription (82.7x real-time) â€” MIGRATED 2025-12-16
- **vLLM**: LLM inference (localhost:8000)
- **NSS-DOCS**: Dual memory system (embeddings + semantic tags)

### Output Dependencies
- **Specifications**: `docs/specs/`
- **Tickets**: `docs/tickets/`
- **Logs**: `docs/logs/`
- **Documentation**: Updated in NSS-DOCS

---

## ğŸ¯ Key Metrics

| Metric | Value |
|--------|-------|
| **Throughput** | 30,000 tokens/sec (prompt) |
| **Generation** | 2,000-2,500 tokens/sec |
| **Concurrency** | 64 parallel requests |
| **Latency** | ~25s for 1,254 files |
| **Recall** | 95-100% |
| **Precision** | ~90% |

---

## ğŸš€ Innovation Highlights

1. **Total Recall**: Binary LLM classification instead of embeddings (~25s, 95-100% recall)
2. **Memory Lite**: Fast embedding search with 200 results, first 20 pre-selected (<1s)
3. **Hypothesis-Driven Search**: 10 interpretations â†’ user selection â†’ refinement
4. **Zero Embeddings**: No indexing required for MVP (Total Recall mode)
5. **Voice-First**: Complete workflow from voice to specification
6. **Recursive Refinement**: 2-3 rounds of search with increasing precision

---

## ğŸ“– See Also

- [Innovation Analysis](VOICEPAL_INNOVATION_ANALYSIS.md) - Detailed evaluation (9.2/10)
- [Mermaid Diagram](voicepal_architecture.mmd) - Technical flow diagram
- [Implementation Plan](../../.gemini/antigravity/brain/6cd9df38-9214-476a-a169-455c8a81d993/implementation_plan.md) - Development roadmap

---

**Status**: MVP Complete âœ…  
**Version**: 3.0  
**Last Updated**: 2025-12-13
