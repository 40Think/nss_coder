---
description: "–°—Ç—Ä–æ–∏—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ knowledge graph –¥–ª—è AI memory system"
date: 2025-12-09
source_file: index_project.py
tags: automation, indexing, embeddings, knowledge-graph
---

# index_project.py - –ü—Å–µ–≤–¥–æ–∫–æ–¥

<!--TAG:pseudo_index_project-->

## PURPOSE
–°—Ç—Ä–æ–∏—Ç —Å–∏—Å—Ç–µ–º—É AI-–ø–∞–º—è—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞, –≤–∫–ª—é—á–∞—é—â—É—é:
- –í–µ–∫—Ç–æ—Ä–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
- Knowledge graph —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—É—â–Ω–æ—Å—Ç—è–º–∏ –∫–æ–¥–∞
- –ë—ã—Å—Ç—Ä—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è lookup-–æ–ø–µ—Ä–∞—Ü–∏–π

## –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–•

### CodeEntity (dataclass)
```pseudo
DATACLASS CodeEntity:
    id: STRING                  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID
    type: STRING                # "file" | "class" | "function"
    name: STRING                # –ò–º—è —Å—É—â–Ω–æ—Å—Ç–∏
    file_path: STRING           # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
    line_number: INT            # –ù–æ–º–µ—Ä —Å—Ç—Ä–æ–∫–∏
    content: STRING             # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ (–∫–æ–¥/docstring)
    embedding: LIST[FLOAT]      # –í–µ–∫—Ç–æ—Ä–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    metadata: DICT              # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
```

### Relationship (dataclass)
```pseudo
DATACLASS Relationship:
    source_id: STRING           # ID –∏—Å—Ö–æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
    target_id: STRING           # ID —Ü–µ–ª–µ–≤–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
    type: STRING                # "imports" | "calls" | "inherits" | "uses"
    metadata: DICT              # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
```

## –ö–õ–ê–°–°: ProjectIndexer

### –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
```pseudo
CLASS ProjectIndexer:
    FUNCTION __init__(project_root):
        self.project_root = project_root
        self.memory_dir = project_root / 'docs' / 'memory'
        self.embeddings_dir = self.memory_dir / 'embeddings'
        self.graph_dir = self.memory_dir / 'knowledge_graph'
        self.indexes_dir = self.memory_dir / 'indexes'  # ‚Üê ADDED (was missing)
        self.entities: DICT[STRING, CodeEntity] = {}  # ‚Üê FIXED (was LIST)
        self.relationships: LIST[Relationship] = []
        
        # –°–æ–∑–¥–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω–µ—Ç
        CREATE self.embeddings_dir IF NOT EXISTS
        CREATE self.graph_dir IF NOT EXISTS
        CREATE self.indexes_dir IF NOT EXISTS  # ‚Üê ADDED
```

### build_embeddings - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
```pseudo
FUNCTION build_embeddings():
    LOG "Building embeddings..."
    
    # –®–∞–≥ 1: –°–æ–±—Ä–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    chunks = []
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    FOR EACH md_file IN GLOB(self.project_root / 'docs', "**/*.md"):
        content = READ md_file
        chunks.APPEND({
            "id": GENERATE_ID(md_file, "doc"),
            "content": content,
            "type": "documentation",
            "file_path": str(md_file)
        })
    
    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å Python-–∫–æ–¥
    FOR EACH py_file IN GLOB(self.project_root / 'processing', "*.py"):
        entities = CALL _extract_code_entities(py_file)
        FOR EACH entity IN entities:
            chunks.APPEND({
                "id": entity.id,
                "content": entity.content,
                "type": entity.type,
                "file_path": entity.file_path
            })
            self.entities.APPEND(entity)
    
    FOR EACH py_file IN GLOB(self.project_root / 'utils', "*.py"):
        entities = CALL _extract_code_entities(py_file)
        # ... –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ
    
    # –®–∞–≥ 2: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (placeholder)
    embeddings = CALL _generate_embeddings_placeholder(chunks)
    
    # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    embeddings_file = self.embeddings_dir / "project_embeddings.json"
    WRITE {
        "count": LENGTH(chunks),
        "chunks": chunks,
        "embeddings_generated": False,  # Placeholder flag
        "timestamp": CURRENT_TIME
    } AS JSON TO embeddings_file
    
    LOG "Prepared {LENGTH(chunks)} chunks for embedding"
```

### build_knowledge_graph - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ knowledge graph
```pseudo
FUNCTION build_knowledge_graph():
    LOG "Building knowledge graph..."
    
    # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∏—Ç—å dependency files
    deps_dir = self.memory_dir / 'dependencies'
    
    IF deps_dir.exists():
        FOR EACH dep_file IN GLOB(deps_dir, "*_dependencies.json"):
            CALL _process_dependency_file(dep_file)
    
    # –®–∞–≥ 2: –ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∏–∑ entities –∏ relationships
    graph = {
        "nodes": [],
        "edges": []
    }
    
    # –î–æ–±–∞–≤–∏—Ç—å —É–∑–ª—ã
    FOR EACH entity IN self.entities:
        graph.nodes.APPEND({
            "id": entity.id,
            "type": entity.type,
            "name": entity.name,
            "file": entity.file_path,
            "line": entity.line_number
        })
    
    # –î–æ–±–∞–≤–∏—Ç—å —Ä—ë–±—Ä–∞
    FOR EACH rel IN self.relationships:
        graph.edges.APPEND({
            "source": rel.source_id,
            "target": rel.target_id,
            "type": rel.type
        })
    
    # –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    graph_file = self.graph_dir / "code_graph.json"  # ‚Üê FIXED (was knowledge_graph.json)
    WRITE graph AS JSON TO graph_file
    
    LOG "Knowledge graph: {LENGTH(graph.nodes)} nodes, {LENGTH(graph.edges)} edges"
```

### build_indexes - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ lookup-–∏–Ω–¥–µ–∫—Å–æ–≤
```pseudo
FUNCTION build_indexes():
    LOG "Building indexes..."
    
    indexes = {
        "by_type": {},      # type -> [entity_ids]
        "by_file": {},      # file_path -> [entity_ids]
        "by_name": {},      # name -> entity_id
        "relationships_from": {},  # entity_id -> [relationships]
        "relationships_to": {}     # entity_id -> [relationships]
    }
    
    # –ò–Ω–¥–µ–∫—Å –ø–æ —Ç–∏–ø—É
    FOR EACH entity IN self.entities:
        IF entity.type NOT IN indexes.by_type:
            indexes.by_type[entity.type] = []
        indexes.by_type[entity.type].APPEND(entity.id)
    
    # –ò–Ω–¥–µ–∫—Å –ø–æ —Ñ–∞–π–ª—É
    FOR EACH entity IN self.entities:
        IF entity.file_path NOT IN indexes.by_file:
            indexes.by_file[entity.file_path] = []
        indexes.by_file[entity.file_path].APPEND(entity.id)
    
    # –ò–Ω–¥–µ–∫—Å –ø–æ –∏–º–µ–Ω–∏
    FOR EACH entity IN self.entities:
        indexes.by_name[entity.name] = entity.id
    
    # –ò–Ω–¥–µ–∫—Å—ã —Å–≤—è–∑–µ–π
    FOR EACH rel IN self.relationships:
        IF rel.source_id NOT IN indexes.relationships_from:
            indexes.relationships_from[rel.source_id] = []
        indexes.relationships_from[rel.source_id].APPEND(rel)
        
        IF rel.target_id NOT IN indexes.relationships_to:
            indexes.relationships_to[rel.target_id] = []
        indexes.relationships_to[rel.target_id].APPEND(rel)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    index_file = self.memory_dir / "indexes" / "lookup_indexes.json"  # ‚Üê FIXED (was indexes.json)
    WRITE indexes AS JSON TO index_file
    
    LOG "Indexes built successfully"
```

### _extract_code_entities - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –∫–æ–¥–∞
```pseudo
FUNCTION _extract_code_entities(file_path):
    entities = []
    
    source = READ file_path
    TRY:
        tree = ast.parse(source)
    CATCH:
        RETURN entities
    
    # –°—É—â–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞
    file_entity = NEW CodeEntity(
        id = CALL _generate_id(file_path, file_path.name, "file"),
        type = "file",
        name = file_path.name,
        file_path = str(file_path),
        line_number = 1,
        content = EXTRACT docstring FROM tree
    )
    entities.APPEND(file_entity)
    
    # –ö–ª–∞—Å—Å—ã –∏ —Ñ—É–Ω–∫—Ü–∏–∏
    FOR EACH node IN ast.walk(tree):
        IF node IS ast.ClassDef:
            class_entity = NEW CodeEntity(
                id = CALL _generate_id(file_path, node.name, "class"),
                type = "class",
                name = node.name,
                file_path = str(file_path),
                line_number = node.lineno,
                content = ast.get_docstring(node) OR ""
            )
            entities.APPEND(class_entity)
        
        IF node IS ast.FunctionDef:
            func_entity = NEW CodeEntity(
                id = CALL _generate_id(file_path, node.name, "function"),
                type = "function",
                name = node.name,
                file_path = str(file_path),
                line_number = node.lineno,
                content = ast.get_docstring(node) OR ""
            )
            entities.APPEND(func_entity)
    
    RETURN entities
```

### _process_dependency_file - –û–±—Ä–∞–±–æ—Ç–∫–∞ dependency JSON
```pseudo
FUNCTION _process_dependency_file(dep_file):
    deps = READ dep_file AS JSON
    
    source_file = deps.file_path
    source_id = CALL _generate_id(source_file, Path(source_file).name, "file")
    
    # –°–æ–∑–¥–∞—Ç—å relationships –∏–∑ imports
    FOR EACH imp IN deps.imports:
        target_module = imp.module
        rel = NEW Relationship(
            source_id = source_id,
            target_id = "module::" + target_module,
            type = "imports"
        )
        self.relationships.APPEND(rel)
    
    # –°–æ–∑–¥–∞—Ç—å relationships –∏–∑ function_calls
    FOR EACH call IN deps.function_calls:
        rel = NEW Relationship(
            source_id = source_id,
            target_id = "func::" + call.name,
            type = "calls"
        )
        self.relationships.APPEND(rel)
    
    # –°–æ–∑–¥–∞—Ç—å relationships –∏–∑ class_hierarchy
    FOR EACH cls IN deps.class_hierarchy:
        FOR EACH base IN cls.bases:
            rel = NEW Relationship(
                source_id = "class::" + cls.name,
                target_id = "class::" + base,
                type = "inherits"
            )
            self.relationships.APPEND(rel)
```

### _generate_id - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ ID
```pseudo
FUNCTION _generate_id(file_path, name, entity_type):
    unique_str = "{file_path}::{entity_type}::{name}"
    hash_val = MD5_HASH(unique_str)[:16]  # ‚Üê FIXED (was [:12])
    RETURN "{entity_type}_{hash_val}"
```

### _generate_embeddings_real - –†–µ–∞–ª—å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (NEW v2025-12-11)
```pseudo
FUNCTION _generate_embeddings_real(chunks) -> LIST[LIST[FLOAT]]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏—Å–ø–æ–ª—å–∑—É—è sentence-transformers –∏–ª–∏ fallback."""
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
    IF chunks IS EMPTY:
        RETURN []
    
    # –ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–∑ –∫–∞–∂–¥–æ–≥–æ chunk
    texts = [c.get('content', '') FOR c IN chunks]
    
    # –ü–æ–ø—ã—Ç–∫–∞ 1: sentence-transformers (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –º–µ—Ç–æ–¥)
    TRY:
        model = SentenceTransformer('all-MiniLM-L6-v2')  # 384-dim embeddings
        embeddings = model.encode(texts, show_progress_bar=True)
        LOG "‚úì Generated {LENGTH(embeddings)} embeddings (dim={embeddings[0].length})"
        RETURN embeddings.tolist()  # Convert numpy to list
    CATCH ImportError:
        LOG WARNING "sentence-transformers not installed, trying vLLM"
    CATCH Exception AS e:
        LOG WARNING "sentence-transformers failed: {e}"
    
    # –ü–æ–ø—ã—Ç–∫–∞ 2: vLLM embedding endpoint (–µ—Å–ª–∏ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω)
    TRY:
        embed_model = docs_config.get("VLLM_MODEL_PATH_EMBEDDING", "")
        IF embed_model AND "path/to/models" NOT IN embed_model:
            LOG "vLLM embedding model configured: {embed_model}"
            # vLLM integration placeholder - –±—É–¥–µ—Ç —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –ø–æ–∑–∂–µ
            LOG WARNING "vLLM embedding not yet implemented, using fallback"
    CATCH Exception AS e:
        LOG WARNING "vLLM check failed: {e}"
    
    # Fallback: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ placeholder —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    LOG WARNING "Using placeholder embeddings (install sentence-transformers for real embeddings)"
    RETURN _generate_embeddings_placeholder(chunks)
```

### _generate_embeddings_placeholder - Placeholder –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
```pseudo
FUNCTION _generate_embeddings_placeholder(chunks) -> LIST[LIST[FLOAT]]:
    """Fallback: –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ placeholder —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö—ç—à–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    
    embeddings = []
    
    FOR EACH chunk IN chunks:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MD5 —Ö—ç—à –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Ç–µ–∫—Å—Ç = –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥)
        content = chunk.get('content', '')
        seed = INT(MD5_HASH(content)[:8], base=16)  # –ü–µ—Ä–≤—ã–µ 8 —Å–∏–º–≤–æ–ª–æ–≤ —Ö—ç—à–∞ –∫–∞–∫ seed
        
        # –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Å–µ–≤–¥–æ—Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Å —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º seed
        RANDOM.seed(seed)
        embedding = [RANDOM.random() FOR _ IN RANGE(384)]  # 384-dim –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å MiniLM
        
        embeddings.APPEND(embedding)
    
    RETURN embeddings
```

### generate_human_readable_index - –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PROJECT_INDEX.md (NEW v2025-12-11)
```pseudo
FUNCTION generate_human_readable_index():
    LOG "Generating human-readable index..."
    
    output = []
    output.APPEND "# Project Index"
    output.APPEND "**Updated**: {CURRENT_TIME}"
    output.APPEND "**Entities**: {LENGTH(entities)} | **Relationships**: {LENGTH(relationships)}"
    
    # –®–∞–≥ 1: –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ —Ñ–∞–π–ª–∞–º
    files_index = GROUP entities BY file_path
    
    # –®–∞–≥ 2: File Structure section
    output.APPEND "## File Structure"
    FOR EACH file_path IN SORTED(files_index.keys()):
        output.APPEND "### [{basename}](file:///{full_path})"
        FOR EACH entity IN SORTED(entities, BY line_number):
            icon = "üèõÔ∏è" IF class ELSE "‚öôÔ∏è" IF function ELSE "üìÑ"
            output.APPEND "- {icon} **{type}** `{name}` (line {line})"
    
    # –®–∞–≥ 3: Mermaid Dependency Graph
    IF relationships NOT EMPTY:
        output.APPEND "## Dependency Graph"
        output.APPEND "```mermaid"
        output.APPEND "graph TD"
        FOR EACH rel IN relationships[:50]:  # Limit for readability
            output.APPEND "    {source[:8]} -->|{type}| {target[:8]}"
        output.APPEND "```"
    
    # –®–∞–≥ 4: Entity Summary
    output.APPEND "## Entity Summary"
    type_counts = COUNT entities BY type
    FOR EACH type, count IN type_counts:
        output.APPEND "- **{type}**: {count}"
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    WRITE output TO memory_dir / "PROJECT_INDEX.md"
    LOG "‚úì Human-readable index saved"
```

## –ö–õ–ê–°–°: IncrementalIndexer (NEW v2025-12-11)
```pseudo
CLASS IncrementalIndexer:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ö—ç—à–µ–π —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    
    FUNCTION __init__(memory_dir):
        self.cache_file = memory_dir / ".index_cache.json"
        self.file_hashes = _load_cache()
        self.changed = False
    
    FUNCTION _load_cache() -> DICT:
        IF cache_file EXISTS:
            RETURN JSON.load(cache_file)
        RETURN {}
    
    FUNCTION save_cache():
        IF self.changed:
            WRITE file_hashes AS JSON TO cache_file
            LOG "‚úì Cache saved: {LENGTH(file_hashes)} file hashes"
    
    FUNCTION get_changed_files(files: LIST[Path]) -> TUPLE[LIST, LIST]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–∫–∏ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö –∏ –Ω–µ–∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
        
        Returns:
            Tuple of (changed_files, unchanged_files)
        """
        changed = []
        unchanged = []
        
        FOR EACH file IN files:
            current_hash = MD5(file.read_bytes())
            cached_hash = file_hashes.GET(str(file))
            
            IF cached_hash != current_hash:
                changed.APPEND(file)
                file_hashes[str(file)] = current_hash
                self.changed = True
            ELSE:
                unchanged.APPEND(file)
        
        RETURN (changed, unchanged)
    
    FUNCTION mark_file_indexed(file: Path):
        """–û—Ç–º–µ—Ç–∏—Ç—å —Ñ–∞–π–ª –∫–∞–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å –µ–≥–æ —Ç–µ–∫—É—â–∏–º —Ö—ç—à–µ–º."""
        TRY:
            current_hash = MD5(file.read_bytes())
            file_hashes[str(file)] = current_hash
            self.changed = True
        CATCH Exception:
            PASS  # –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞
```

## CLI INTERFACE (UPDATED v2025-12-11)
```pseudo
ARGUMENTS:
    --build-embeddings      # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    --build-knowledge-graph # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å knowledge graph
    --build-indexes         # –ü–æ—Å—Ç—Ä–æ–∏—Ç—å lookup –∏–Ω–¥–µ–∫—Å—ã
    --build-human-index     # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å PROJECT_INDEX.md (NEW)
    --build-all             # –í—Å—ë –≤–º–µ—Å—Ç–µ
    --incremental           # –¢–æ–ª—å–∫–æ –∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (NEW)

ENTRY POINT main():
    PARSE arguments
    project_root = DETECT from script location
    
    indexer = NEW ProjectIndexer(project_root)
    
    IF args.incremental:
        LOG "üì¶ Incremental mode enabled"
    
    IF args.build_all OR args.build_embeddings:
        indexer.build_embeddings()
    
    IF args.build_all OR args.build_knowledge_graph:
        indexer.build_knowledge_graph()
    
    IF args.build_all OR args.build_indexes:
        indexer.build_indexes()
    
    IF args.build_all OR args.build_human_index:
        indexer.generate_human_readable_index()
    
    LOG "‚úì Indexing complete!"
```

## –ó–ê–í–ò–°–ò–ú–û–°–¢–ò
- ast (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞)
- pathlib.Path
- dataclasses
- hashlib
- datetime
- sentence_transformers (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
- docs.utils.docs_logger.DocsLogger  # ‚Üê FIXED (was utils.paranoid_logger)
- docs.utils.docs_config.docs_config  # ‚Üê ADDED

<!--/TAG:pseudo_index_project-->

