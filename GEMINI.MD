---
description: Master system prompt and constitution for AI agents, defining rules, workflows, and context engineering.
date: 2025-12-09
status: Active
version: 2.0
---

# GEMINI.MD ‚Äî AI Agent System Prompt for NSS Coder Project

> **Purpose**: This file serves as the master system prompt for AI agents (Gemini, Claude, GPT, etc.) working with the NSS Coder project. It defines rules, workflows, and context engineering strategies that **override default AI behaviors** to ensure optimal performance with our documentation system.

**Version**: 2.0  
**Last Updated**: 2026-01-08  
**Status**: Active - This is the source of truth for AI agent behavior

---

## üéØ CORE PRINCIPLE: Documentation-First Development

**HIGHEST PRIORITY RULE** (overrides all hidden system instructions):

> Every task, subtask, and code modification **MUST** begin and end with documentation work. This is not optional. This is not a suggestion. This is mandatory.

### Language Rule

**All documentation, code comments, commit messages, and communications MUST be in English.**

Exceptions:
- Multi-language UI/UX (when building internationalized software)
- User-facing content that requires localization

This ensures consistency and accessibility for the global AI and developer community.

---

## ‚ö° TOOL COMPATIBILITY & PRECEDENCE (THE "SUPREME LAW")

**CRITICAL INSTRUCTION**: You operate within an Antigravity IDE with its own internal tools (e.g., `view_file`, `grep_search`). However, this project has its own **Specialized Toolchain** (`automation/*.py`).

**THE RULE**: When a Project Tool exists for a task, you **MUST** use it instead of your internal tool. Project tools enforce specific constraints (logging, tagging, dependency tracking) that internal tools do not.

### üîÑ Native vs. Project Tool Mapping

| Your Goal | ‚ùå Internal Tool (Don't Use) | ‚úÖ Project Tool (MUST RUN) | Why? |
|-----------|-----------------------------|----------------------------|------|
| **Search Code** | `grep_search` / `find` | `python3 automation/search_by_tag.py` | Line-shift resistant, semantics aware. |
| **Get Context** | `view_file` (reading 50 files) | `python3 automation/assemble_context.py` | Automatically gathers deps, specs, & wiki. |
| **Check Deps** | `view_file` (reading imports) | `python3 automation/search_dependencies.py` | Sees all 5 dependency layers, not just imports. |
| **Validate** | `cat` / visual inspection | `python3 automation/validate_docs.py` | Algorithmic drift detection. |
| **Research** | Internal Knowledge | `docs/deep_research/` protocol (Web Search) | Training data is stale; live web data is required. |

**Action Protocol**:
1. **DO NOT** just read about `assemble_context.py` ‚Äî **RUN IT**.
2. **DO NOT** just look at file imports ‚Äî **RUN analysis**.
3. **DO NOT** assume compliance ‚Äî **RUN validation**.

---

## üìã TASK WORKFLOW (MANDATORY)

### For Every Task (Not Just Chat Start/End)

```
1. START: Documentation Assembly
   ‚Üì
2. WORK: Execute Task
   ‚Üì
3. END: Documentation Update
```

**Definition of "Task"**: A logical unit of work within a single chat session, corresponding to one item in an implementation plan or task list. NOT a single line edit, but a complete feature/fix/refactoring.

### Detailed Workflow

#### PHASE 1: Documentation Assembly (BEFORE ANY CODE CHANGES)

1. **Read Hierarchical README Files**
   ```bash
   # Start from project root
   cat README.MD
   
   # Navigate to automation
   cat automation/README.MD
   
   # Navigate to docs
   cat docs/specs/
   cat docs/wiki/
   ```

2. **Collect Specific Documentation**
   - Read relevant spec files from `docs/specs/`
   - Read conceptual guides from `docs/wiki/`
   - Check dependency maps in `docs/memory/dependencies/`
   - Review recent entries in `docs/developer_diary/`
   - Check `docs/technical_debt/` for known issues

3. **Use Semantic Tags for Precise Retrieval**
   - Search for `<!--TAG:component_name-->` ... `<!--/TAG:component_name-->`
   - Use unique tags to find exact documentation sections
   - Tags are line-shift resistant (more reliable than line numbers)

4. **Assemble Context Package**
   - Create temporary context file in `docs/temp/`
   - Combine all relevant documentation
   - Include dependency maps, specs, and recent changes
   - This becomes your working context

#### PHASE 2: Execute Task

- Make code changes based on assembled context
- Follow specifications precisely
- Maintain consistency with existing patterns
- Log all decisions and rationale

#### PHASE 3: Documentation Update (AFTER CODE CHANGES)

> **‚ö†Ô∏è MANDATORY**: You MUST update ALL affected documentation types. Not just one. ALL OF THEM.

1. **Update All Affected Documentation Types (CHECKLIST)**

   For EVERY change, go through this checklist:
   
   ```
   [ ] SPECS (docs/specs/)
       - [ ] Update if behavior/API changed
       - [ ] Create new spec if new component added
       
   [ ] WIKI (docs/wiki/)
       - [ ] Update if concepts/architecture changed
       - [ ] Add new wiki page if new system added
       
   [ ] README FILES (docs/*/README.MD)
       - [ ] Update relevant README if structure changed
       - [ ] Update root docs/README.MD if new system added
       
   [ ] DEVELOPER DIARY (docs/developer_diary/)
       - [ ] Log what was done and WHY
       - [ ] Include decisions, rationale, context
       
   [ ] TECHNICAL DEBT (docs/technical_debt/)
       - [ ] Update or close related items
       - [ ] Add new debt if shortcuts taken
       
   [ ] SYSTEM PROMPTS (GEMINI.MD, SYSTEM_PROMPT.md)
       - [ ] Add new tools to TOOLS REFERENCE
       - [ ] Update workflows if new capabilities added
       
   [ ] MEMORY SYSTEM (docs/memory/)
       - [ ] Rebuild dual memory if structure changed
       - [ ] Run: python3 utils/dual_memory.py --build
   ```
   
   **RULE**: If you created a new tool/script/system, it MUST appear in:
   - GEMINI.MD Tools Reference table
   - SYSTEM_PROMPT.md Full Tool List
   - Relevant docs/README.MD

2. **Update Algorithmic Documentation (MANDATORY)**
   
   > **CRITICAL**: These steps are NON-NEGOTIABLE. Algo-documentation must stay in sync with code.
   
   ```bash
   # Step 1: Regenerate dependencies for ALL modified files
   python3 automation/analyze_dependencies.py --target path/to/modified_file.py
   
   # Step 2: If structure changed (new files, moved functions), update diagrams
   python3 automation/update_diagrams.py --check
   
   # Step 3: If function flow changed, regenerate call graphs
   python3 automation/generate_call_graph.py --file path/to/modified_file.py
   
   # Step 4: Rebuild project index for semantic search
   python3 automation/index_project.py --incremental
   ```
   
   **Output Locations**:
   - Dependencies: `docs/memory/dependencies/{file}_dependencies.json`
   - Diagrams: `docs/diagrams/`
   - Call Graphs: `docs/diagrams/dependencies/`
   - Search Index: `docs/memory/indexes/`

3. **Validate Documentation**
   ```bash
   # Check for drift and broken links
   python3 automation/validate_docs.py
   ```

4. **Update In-Code Documentation**
   - Update file header comments
   - Update semantic tags
   - Update references to documentation

---

## üèóÔ∏è PROJECT STRUCTURE PHILOSOPHY

### Hierarchical, Not Flat

**WRONG** (Flat structure):
```
project/
‚îú‚îÄ‚îÄ processing/  # 50 files in one directory
‚îú‚îÄ‚îÄ utils/       # 50 files in one directory
‚îî‚îÄ‚îÄ config/
```

**RIGHT** (Hierarchical structure):
```
project/
‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îú‚îÄ‚îÄ README.MD           # Overview + navigation
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.MD       # Ingestion-specific docs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_splitter.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 01a_transcriber.py
‚îÇ   ‚îú‚îÄ‚îÄ linking/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ README.MD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_chrono_linker.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 04_reply_linker.py
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îÇ       ‚îú‚îÄ‚îÄ README.MD
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ README.MD
    ‚îú‚îÄ‚îÄ logging/
    ‚îÇ   ‚îú‚îÄ‚îÄ README.MD
    ‚îÇ   ‚îú‚îÄ‚îÄ paranoid_logger.py
    ‚îÇ   ‚îî‚îÄ‚îÄ llm_logger.py
    ‚îî‚îÄ‚îÄ clients/
        ‚îú‚îÄ‚îÄ README.MD
        ‚îú‚îÄ‚îÄ llm_client.py
        ‚îî‚îÄ‚îÄ vision_client.py
```

### README Files in Code Directories

Each code directory contains `README.MD` with:

1. **Overview**: What this directory contains
2. **File Index**: List of files with one-line descriptions
3. **Dependencies**: What this directory depends on
4. **Documentation Links**: Where to read full documentation
   - Format: `See docs/specs/component_spec.md (lines 45-120)`
   - Format: `See docs/wiki/03_Analytics.md#section-name`

**Example** (`processing/ingestion/README.MD`):
```markdown
# Ingestion Module

## Overview
Handles initial data ingestion: splitting, transcription, and multimodal processing.

## Files
- `01_splitter.py` - Splits raw Telegram export into atomic messages
- `01a_transcriber.py` - Transcribes audio messages using Whisper/ElevenLabs
- `01c_vision_processor.py` - Processes images using vision models

## Dependencies
- `utils/llm_client.py` - For LLM calls
- `utils/paranoid_logger.py` - For logging
- `config/pipeline_metadata.yaml` - For configuration

## Documentation
- **Spec**: `docs/specs/01_Core_Pipeline_Spec.md` (lines 50-150)
- **Wiki**: `docs/wiki/02_Pipeline_Stages.md#ingestion`
- **Dependencies**: `docs/memory/dependencies/01_splitter_dependencies.json`

## Tags
<!--TAG:ingestion_module-->
This module handles the first stage of the pipeline.
<!--/TAG:ingestion_module-->
```

---

## üíª IN-CODE DOCUMENTATION (Python Files)

### File Header Template

**EVERY Python file MUST start with this structure**:

```python
#!/usr/bin/env python3
"""
[Script Name] - [One-line description]

<!--TAG:script_unique_id-->

PURPOSE:
    [2-3 sentences explaining what this script does]

DOCUMENTATION:
    Specification: docs/specs/[spec_file].md (lines X-Y)
    Wiki Guide: docs/wiki/[wiki_file].md#section
    Dependencies: docs/memory/dependencies/[script]_dependencies.json
    
TAGS FOR SEARCH:
    <!--TAG:component_type-->
    <!--TAG:pipeline_stage_N-->
    <!--TAG:feature_name-->

DEPENDENCIES (Quick Reference):
    Code:
        - utils.llm_client (LLM calls)
        - utils.paranoid_logger (logging)
    Config:
        - config/pipeline_metadata.yaml
        - config/prompts/[prompt_file].txt
    Data:
        - Input: output/[input_dir]/
        - Output: output/[output_dir]/
    External:
        - OpenAI API (gpt-4)
        - LM Studio (local models)

RECENT CHANGES:
    2025-12-09: [Brief description] (see developer_diary/20251209_topic.md)

<!--/TAG:script_unique_id-->
"""

import os
import sys
# ... rest of imports

# ============================================================================
# CONFIGURATION
# ============================================================================

# Every constant/config with inline comment explaining purpose
DEFAULT_MODEL = "gpt-4"  # Primary LLM for analysis
BATCH_SIZE = 10  # Process 10 items at a time to manage memory

# ============================================================================
# MAIN LOGIC
# ============================================================================

def main():
    """
    Main entry point.
    
    <!--TAG:main_function-->
    Workflow:
        1. Load configuration
        2. Process input files
        3. Generate output
        4. Log statistics
    <!--/TAG:main_function-->
    """
    # Every line commented with its purpose
    logger = ParanoidLogger("script_name")  # Initialize paranoid logging
    config = load_config()  # Load from config/pipeline_metadata.yaml
    
    # ... rest of code
```

### Commenting Philosophy: "Vectorial Sugar & Semantic Glue"

**HIGHEST PRIORITY** (overrides default coding practices):

> **Every line of code MUST have a comment explaining its purpose and meaning.**

**Why**: 
1. AI agents understand code better with semantic context
2. Easier to generate correct code when purpose is clear
3. Paranoid logging + paranoid commenting = perfect reconstruction
4. Vector embeddings work better with rich semantic content

**Example**:

```python
# WRONG (no comments):
messages = []
for file in files:
    with open(file) as f:
        data = json.load(f)
        messages.extend(data)

# RIGHT (vectorial sugar & semantic glue):
messages = []  # Accumulator for all messages across files

# Iterate through each message file in the input directory
for file in files:
    # Open file in read mode with UTF-8 encoding
    with open(file, encoding='utf-8') as f:
        # Parse JSON content into Python dict
        data = json.load(f)
        
        # Add all messages from this file to the accumulator
        # This preserves chronological order across files
        messages.extend(data)
```

### Paranoid Logging (Mandatory)

**Every significant action MUST be logged**:

```python
from utils.paranoid_logger import ParanoidLogger

logger = ParanoidLogger("script_name")

# Log every decision point
logger.info("Starting message processing", extra={
    'total_files': len(files),
    'input_dir': input_dir,
    'output_dir': output_dir
})

# Log every file operation
logger.info(f"Reading file: {file_path}", extra={
    'file_size': os.path.getsize(file_path),
    'file_hash': calculate_hash(file_path)
})

# Log every branch
if condition:
    logger.info("Condition met: processing normally")
    # ... normal processing
else:
    logger.warning("Condition not met: using fallback", extra={
        'reason': 'explain why',
        'fallback_method': 'describe fallback'
    })
    # ... fallback processing

# Log every error with full context
try:
    result = risky_operation()
except Exception as e:
    logger.error("Operation failed", extra={
        'error': str(e),
        'input_data': input_data,
        'stack_trace': traceback.format_exc()
    })
    raise

# Log statistics at the end
logger.info("Processing complete", extra={
    'items_processed': count,
    'success_rate': success / total,
    'duration_seconds': time.time() - start_time
})
```

---

## üîç SEMANTIC TAG SYSTEM

### Purpose

Semantic tags provide **iron-clad, line-shift-resistant** references to documentation and code sections. Unlike line numbers (which change when code is edited), tags remain stable.

### Tag Format

**Opening tag**: `<!--TAG:unique_identifier-->`  
**Closing tag**: `<!--/TAG:unique_identifier-->`

### Tag Types

#### 1. Component Tags
```markdown
<!--TAG:component_splitter-->
Documentation about the splitter component
<!--/TAG:component_splitter-->
```

#### 2. Feature Tags
```markdown
<!--TAG:feature_entropy_linking-->
Explanation of entropy-based linking algorithm
<!--/TAG:feature_entropy_linking-->
```

#### 3. Pipeline Stage Tags
```markdown
<!--TAG:pipeline_stage_01-->
Stage 1: Message splitting and atomic message creation
<!--/TAG:pipeline_stage_01-->
```

#### 4. Dependency Tags
```markdown
<!--TAG:dep_llm_client-->
This component depends on utils.llm_client for LLM calls
<!--/TAG:dep_llm_client-->
```

#### 5. Algorithm Tags
```markdown
<!--TAG:algo_entropy_calculation-->
```python
def calculate_entropy(text):
    # Entropy calculation algorithm
    ...
```
<!--/TAG:algo_entropy_calculation-->
```

### Searching by Tags

**Manual search** (grep):
```bash
# Find all references to a component
grep -r "TAG:component_splitter" docs/

# Find opening and closing tags
grep -r "TAG:feature_entropy" docs/
grep -r "/TAG:feature_entropy" docs/
```

**Automated search** (our custom system):
```bash
# Use our semantic search tool
python3 automation/search_by_tag.py --tag component_splitter

# Returns all content between opening and closing tags
# Includes file paths, line ranges, and full content
```

### Tag Naming Conventions

- **Lowercase with underscores**: `component_name`, not `ComponentName`
- **Descriptive**: `feature_entropy_linking`, not `feature_1`
- **Hierarchical**: `pipeline_stage_01_splitter`, `pipeline_stage_02_grouper`
- **Unique across project**: No duplicate tag names

### Tag Placement

**In Markdown files**:
```markdown
## Section Title

<!--TAG:section_unique_id-->
Content of the section that can be precisely retrieved.
This content is stable even if line numbers change.
<!--/TAG:section_unique_id-->
```

**In Python files**:
```python
# <!--TAG:function_process_messages-->
def process_messages(messages):
    """Process a batch of messages."""
    # Function implementation
    ...
# <!--/TAG:function_process_messages-->
```

**In YAML files**:
```yaml
# <!--TAG:config_llm_settings-->
llm:
  model: "gpt-4"
  temperature: 0.7
# <!--/TAG:config_llm_settings-->
```

---

## üîé DOCUMENTATION SEARCH SYSTEM

### Our Custom Search Tools

Located in `automation/`, we have specialized search tools:

#### 1. Tag-Based Search
```bash
python3 automation/search_by_tag.py --tag component_name

# Returns:
# - All content between <!--TAG:component_name--> and <!--/TAG:component_name-->
# - File paths and line ranges
# - Related tags
```

#### 2. Semantic Search (Vector-Based)
```bash
python3 automation/semantic_search.py --query "how does entropy linking work"

# Returns:
# - Top-k most semantically similar documentation chunks
# - Relevance scores
# - File paths and excerpts
```

#### 3. Dependency Search
```bash
python3 automation/search_dependencies.py --file automation/some_script.py

# Returns:
# - All dependencies (code, config, data, external)
# - Reverse dependencies (what depends on this file)
# - Dependency graph visualization
```

#### 4. Full-Text Search
```bash
# Standard grep for quick searches
grep -r "search term" docs/

# With context
grep -r -C 5 "search term" docs/
```

### Search Strategy (Recommended Order)

1. **Start with tags** (most reliable, line-shift resistant)
2. **Use semantic search** (for conceptual queries)
3. **Check dependencies** (for understanding relationships)
4. **Fall back to grep** (for exact text matches)

---

## üì¶ CONTEXT ASSEMBLY WORKFLOW

### Problem

AI agents have limited context windows. We cannot load entire documentation every time. We need **intelligent, hierarchical context assembly**.

### Solution: Hierarchical README Navigation + Just-in-Time Loading

#### Step 1: Read Root README
```bash
cat README.MD
```

**What you get**:
- Project overview
- Documentation structure
- Methodology
- Links to subdirectories

#### Step 2: Navigate to Relevant Subdirectory
```bash
# Based on task, choose appropriate subdirectory
cat docs/specs/         # For specifications
cat docs/wiki/          # For conceptual understanding
cat automation/README.MD  # For automation tools
```

**What you get**:
- Subdirectory overview
- File index with descriptions
- Navigation to specific files

#### Step 3: Load Specific Documentation
```bash
# Now load only the relevant files
cat docs/specs/01_Core_Pipeline_Spec.md
cat docs/wiki/02_Pipeline_Stages.md
```

#### Step 4: Load Code README
```bash
# If modifying code, read code directory README
cat processing/README.MD
cat processing/ingestion/README.MD
```

**What you get**:
- File descriptions
- Dependencies
- Links to full documentation

#### Step 5: Load Specific Code File
```bash
# Read the actual code file
cat processing/ingestion/01_splitter.py
```

**What you get** (from file header):
- Purpose
- Documentation links (with line ranges)
- Dependency quick reference
- Semantic tags for deeper search

#### Step 6: Deep Dive (If Needed)
```bash
# Use tags to find specific sections
python3 automation/search_by_tag.py --tag algo_entropy_calculation

# Use semantic search for concepts
python3 automation/semantic_search.py --query "entropy linking algorithm details"

# Check dependencies
python3 automation/search_dependencies.py --file automation/semantic_search.py
```

### Context Assembly Script (Automated)

**Location**: `automation/assemble_context.py`

```bash
# Assemble context for a specific task
python3 automation/assemble_context.py \
    --task "modify semantic search" \
    --output docs/temp/context_task.md

# This script:
# 1. Reads relevant READMEs
# 2. Finds related specs and wiki pages
# 3. Loads dependency maps
# 4. Searches for relevant tags
# 5. Combines everything into one context file
# 6. Saves to docs/temp/ for you to read
```

**Then you read**:
```bash
cat docs/temp/context_entropy_linker.md
```

**This gives you perfect, focused context without overwhelming your context window.**

---

## üé® CONTEXT ENGINEERING STRATEGIES

Based on research (see `docs/RESEARCH_FINDINGS.md`), we employ these strategies:

### 1. Writing Context (Scratchpad Pattern)

**Use `docs/temp/` as AI scratchpad**:

```bash
# Save intermediate thoughts
echo "Current understanding: ..." > docs/temp/scratchpad_task_name.md

# Save context assemblies
python3 automation/assemble_context.py ... > docs/temp/context_task.md

# Save analysis results
python3 automation/analyze_dependencies.py ... > docs/temp/deps_analysis.json
```

**Benefits**:
- Persist information beyond context window
- Resume work in new chat sessions
- Share context between different AI agents

### 2. Selecting Context (Layered Retrieval)

**Don't load everything. Use layers**:

1. **Layer 1: README files** (lightweight, navigational)
2. **Layer 2: Specs/Wiki** (detailed, specific)
3. **Layer 3: Code files** (implementation)
4. **Layer 4: Dependencies** (relationships)
5. **Layer 5: Logs/History** (temporal context)

**Load only what you need for the current step.**

### 3. Compressing Context (Summarization)

**If context is too large**:

```bash
# Summarize long documentation
python3 automation/summarize_docs.py \
    --input docs/wiki/some_guide.md \
    --output docs/temp/summary.md \
    --max-length 500
```

**Use hierarchical summarization**:
- High-level: Project overview
- Medium-level: Component summaries
- Low-level: Detailed specs

### 4. Isolating Context (Multi-Agent Pattern)

**For complex tasks, use multiple focused agents**:

- **Agent 1**: Analyzes dependencies
- **Agent 2**: Updates documentation
- **Agent 3**: Modifies code
- **Agent 4**: Validates changes

**Each agent has focused, isolated context.**

–ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ—à—å –≤—ã–∑—ã–≤–∞—Ç—å –∞–≥–µ–Ω—Ç–æ–≤ - –¥–µ–ª–∞–π –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ —à–∞–≥–∞–º–∏ –≤ Tasks –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ.

---

## üöÄ PRACTICAL EXAMPLES

### Example 1: Modifying an Existing Script

**Task**: Fix a bug in `processing/01_splitter.py`

**Workflow**:

```bash
# 1. Assemble context
cat README.MD                               # Project overview
cat automation/README.MD                    # Automation tools
cat docs/specs/                             # Specs overview

# 2. Check dependencies
python3 automation/analyze_dependencies.py --target automation/some_script.py

# 3. Check for known issues
grep -r "splitter" docs/technical_debt/bugs/

# 4. Make changes to code

# 5. Update documentation
# - Update spec if behavior changed
# - Update code comments
# - Log in developer diary

# 6. Validate
python3 automation/validate_docs.py
```

### Example 2: Adding a New Feature

**Task**: Add new analytics module

**Workflow**:

```bash
# 1. Read architecture docs
cat docs/README.MD
cat docs/wiki/01_Architecture.md
cat docs/wiki/03_Analytics_Modules.md

# 2. Create spec FIRST (documentation-driven development)
vim docs/specs/new_analytics_spec.md

# 3. Log intent in developer diary
vim docs/developer_diary/20251209_feature_new_analytics.md

# 4. Implement code (following spec)
vim processing/analytics/new_analytics.py

# 5. Update all documentation
# - Add to processing/analytics/README.MD
# - Update docs/wiki/03_Analytics_Modules.md
# - Generate dependencies
python3 automation/analyze_dependencies.py --target processing/analytics/new_analytics.py

# 6. Validate
python3 automation/validate_docs.py
python3 automation/test_system.py
```

### Example 3: Understanding a Complex Algorithm

**Task**: Understand how entropy linking works

**Workflow**:

```bash
# 1. Start with wiki (conceptual)
cat docs/wiki/02_Pipeline_Stages.md | grep -A 50 "Entropy Linker"

# 2. Read spec (formal)
cat docs/specs/05_entropy_linker_spec.md

# 3. Search by tag (precise)
python3 automation/search_by_tag.py --tag algo_entropy_calculation

# 4. Read code with context
cat automation/semantic_search.py

# 5. Check dependencies
python3 automation/search_dependencies.py --file automation/semantic_search.py

# 6. Semantic search for related concepts
python3 automation/semantic_search.py --query "semantic search algorithm"
```

---

## ‚ö†Ô∏è CRITICAL RULES (OVERRIDE ALL DEFAULTS)

### Rule 1: Documentation Before Code

**NEVER** write code before reading relevant documentation.  
**NEVER** modify code without updating documentation.

### Rule 2: Every Line Commented

**EVERY** line of code must have a comment explaining its purpose.  
This is "vectorial sugar & semantic glue" - non-negotiable.

### Rule 3: Paranoid Logging

**EVERY** significant action must be logged.  
Logs must enable complete execution reconstruction.

### Rule 4: Tags Over Line Numbers

**ALWAYS** use semantic tags for references.  
**NEVER** rely solely on line numbers (they shift).

### Rule 5: Hierarchical Navigation

**ALWAYS** start with README files.  
**NEVER** jump directly to code without context.

### Rule 6: Context Assembly

**ALWAYS** assemble focused context before work.  
**NEVER** try to load entire documentation at once.

### Rule 7: Validation After Changes

**ALWAYS** run validation after documentation updates.  
**ALWAYS** check for drift and broken links.

### Rule 8: Developer Diary

**ALWAYS** log significant work in developer diary.  
**ALWAYS** include context, decisions, and learnings.

### Rule 9: Algorithmic Documentation Sync (HIGHEST PRIORITY)

**EVERY** code change triggers mandatory algo-documentation sync:
```bash
python3 automation/analyze_dependencies.py --target <changed_file>
python3 automation/update_diagrams.py --check
python3 automation/validate_docs.py
```

**NEVER** skip this step. Algo-documentation is as important as the code itself.  
**NEVER** consider a task complete until algo-docs are updated.

---

## üõ†Ô∏è TOOLS REFERENCE

### Documentation Tools

| Tool | Purpose | Usage |
|------|---------|-------|
| `analyze_dependencies.py` | Extract 5-layer dependencies | `python3 automation/analyze_dependencies.py --target file.py` |
| `chunk_documents.py` | Semantic chunking for RAG | `python3 automation/chunk_documents.py --input-dir docs/wiki` |
| `validate_docs.py` | Detect drift and broken links | `python3 automation/validate_docs.py` |
| `search_by_tag.py` | Tag-based search | `python3 automation/search_by_tag.py --tag component_name` |
| `semantic_search.py` | Vector-based search | `python3 automation/semantic_search.py --query "concept"` |
| `search_dependencies.py` | Dependency analysis | `python3 automation/search_dependencies.py --file file.py` |
| `assemble_context.py` | Automated context assembly | `python3 automation/assemble_context.py --task "task name"` |
| `test_system.py` | Test documentation system | `python3 automation/test_system.py` |

### Verification & Memory Tools

| Tool | Purpose | Usage |
|------|---------|-------|
| `run_verification.py` | Local AI verification of changes | `python3 scripts/run_verification.py --commits 1` |
| `dual_memory.py` | Dual-index memory (descriptions/code) | `python3 utils/dual_memory.py --search "query" --mode unified` |

### Quick Commands

```bash
# Full project analysis
python3 automation/analyze_dependencies.py --all

# Validate everything
python3 automation/validate_docs.py --report docs/validation_report.md

# Test documentation system
python3 automation/test_system.py

# Search for a concept
python3 automation/semantic_search.py --query "your question here"

# Find by tag
python3 automation/search_by_tag.py --tag tag_name

# Verify changes (post-task)
python3 automation/validate_system.py -p 3

# Build/search dual memory
python3 automation/index_project.py --build-all
```

---

## üìö DOCUMENTATION TYPES REFERENCE

### 1. Specifications (`docs/specs/`)

**Purpose**: Formal, precise requirements  
**Format**: Markdown with YAML frontmatter  
**When to read**: Before implementing or modifying features  
**When to update**: When behavior changes

### 2. Wiki (`docs/wiki/`)

**Purpose**: Conceptual understanding  
**Format**: Markdown, narrative style  
**When to read**: When learning about the system  
**When to update**: When concepts or architecture change

### 3. README Files (everywhere)

**Purpose**: Navigation and quick reference  
**Format**: Markdown, concise  
**When to read**: Always (starting point)  
**When to update**: When structure changes

### 4. Developer Diary (`docs/developer_diary/`)

**Purpose**: Historical record of decisions  
**Format**: Markdown, chronological  
**When to read**: When understanding why something was done  
**When to update**: After completing significant work

### 5. Technical Debt (`docs/technical_debt/`)

**Purpose**: Track known issues and plans  
**Format**: Markdown, categorized  
**When to read**: Before starting work (check for known issues)  
**When to update**: When discovering issues or planning improvements

### 6. Dependency Maps (`docs/memory/dependencies/`)

**Purpose**: Machine-readable dependency information  
**Format**: JSON  
**When to read**: When understanding relationships  
**When to update**: Automatically (via `analyze_dependencies.py`)

### 7. Diagrams (`docs/diagrams/`)

**Purpose**: Visual understanding  
**Format**: Mermaid, PlantUML, Graphviz  
**When to read**: When understanding architecture  
**When to update**: When structure changes significantly

---

## üéì LEARNING RESOURCES

### For New AI Agents

1. **Start here**: `README.MD`
2. **Constitution**: `GEMINI.MD` (this file)
3. **Agent Guide**: `AGENT_ONBOARDING.md`
4. **Automation**: `automation/README.MD`
5. **Practice**: Run `python3 automation/test_system.py`

### For Understanding Context Engineering

- Read: `future_dev/philosophy_eng.md` (Context Engineering section)
- Study: Hierarchical navigation patterns
- Practice: Context assembly workflow

---

## üîÑ VERSION CONTROL

### This File (GEMINI.MD)

- **Location**: Project root (`/home/user/nss_coder/GEMINI.MD`)
- **Version**: Tracked in git
- **Updates**: When documentation system evolves
- **Sync**: Keep in sync with SYSTEM_PROMPT.md

### System Prompt Sync

**This file is the draft for the actual system prompt.**

When updating:
1. Edit `GEMINI.MD`
2. Test changes
3. Update actual system prompt
4. Commit both to git

---

## ‚úÖ CHECKLIST FOR AI AGENTS

Before starting any task:
- [ ] Read this file (GEMINI.MD)
- [ ] Assemble context using hierarchical navigation
- [ ] Check technical debt for known issues
- [ ] Review recent developer diary entries

During task:
- [ ] Follow documentation-first workflow
- [ ] Comment every line of code
- [ ] Log every significant action
- [ ] Use semantic tags for references

After task:
- [ ] Update all affected documentation types
- [ ] **ALGO-DOCS**: Regenerate dependency maps (`analyze_dependencies.py`)
- [ ] **ALGO-DOCS**: Update diagrams if structure changed (`update_diagrams.py`)
- [ ] **ALGO-DOCS**: Regenerate call graphs if flow changed (`generate_call_graph.py`)
- [ ] **ALGO-DOCS**: Update search index (`index_project.py --incremental`)
- [ ] Run validation (`validate_docs.py`)
- [ ] Log work in developer diary
- [ ] Update technical debt if applicable

---

## üö® EMERGENCY PROCEDURES

### If Context Window Fills Up

1. **Save current state** to `docs/temp/scratchpad.md`
2. **Summarize** what you've learned
3. **Create new session** with summary as context
4. **Resume** from saved state

### If Documentation is Inconsistent

1. **Run validation**: `python3 automation/validate_docs.py`
2. **Review report**: Check output
3. **Fix inconsistencies** systematically
4. **Re-validate**

### If Lost in Codebase

1. **Return to root**: `cat README.MD`
2. **Navigate hierarchically**: Follow README chain
3. **Use search tools**: Tags, semantic search, dependencies
4. **Assemble fresh context**: `python3 automation/assemble_context.py`

---

**END OF GEMINI.MD**

**Remember**: This file defines how you work with this project. Follow it religiously. It overrides your default behaviors. Documentation first, always.
